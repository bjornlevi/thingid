#!/usr/bin/env python3
"""
Convert cached XML into per-table CSVs (bronze layer) without touching the DB.

Usage examples:
  PYTHON=.venv/bin/python scripts/mint_bronze.py --cache-dir data/cache --outdir data/bronze
  PYTHON=.venv/bin/python scripts/mint_bronze.py --cache-dir data/cache --outdir data/bronze --lthing 154
  PYTHON=.venv/bin/python scripts/mint_bronze.py --cache-dir data/cache --outdir data/bronze --lthing-range 150,154
  PYTHON=.venv/bin/python scripts/mint_bronze.py --cache-dir data/cache --outdir data/bronze --all-lthing
"""

from __future__ import annotations

import argparse
import csv
import json
import os
from pathlib import Path
from typing import Any, Dict, List

import sys

ROOT = Path(__file__).resolve().parent.parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from scripts.get_data import (  # type: ignore
    Fetcher,
    discover_current_lthing_and_yfirlit,
    discover_lthing_and_yfirlit,
    iter_leaf_fields,
    iter_records,
    list_lthing_sessions,
    norm_url,
    parse_xml,
)


def resource_urls_for_lthing(base_resources: List[Dict[str, Any]], lthing_val: int, yfirlit_urls: Dict[str, str]) -> List[Dict[str, Any]]:
    out = []
    for r in base_resources:
        rcopy = dict(r)
        name = rcopy.get("name")
        if name and name in yfirlit_urls:
            rcopy["url"] = yfirlit_urls[name]
        else:
            url = rcopy.get("url", "")
            if "lthing=" in url:
                rcopy["url"] = norm_url("", url.replace("lthing=", f"lthing={lthing_val}"))
        out.append(rcopy)
    return out


def write_csv(path: Path, rows: List[Dict[str, Any]], headers: List[str]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        for row in rows:
            writer.writerow({h: row.get(h) for h in headers})


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--schema", default="schema_map.json", help="schema_map.json generated by check_data.py")
    ap.add_argument("--cache-dir", default="data/cache", help="Base cache directory")
    ap.add_argument("--outdir", default="data/bronze", help="Output directory for CSVs")
    ap.add_argument("--lthing", type=int, default=None, help="Specific löggjafarþing number")
    ap.add_argument("--lthing-range", type=str, default=None, help="Inclusive range start,end (e.g. 150,154)")
    ap.add_argument("--all-lthing", action="store_true", help="Process all löggjafarþing")
    ap.add_argument("--sleep", type=float, default=0.0, help="Sleep between requests (cache misses)")
    ap.add_argument("--force-fetch", action="store_true", help="Fetch from network even if cached")
    args = ap.parse_args()

    with open(args.schema, "r", encoding="utf-8") as f:
        schema_map = json.load(f)
    resources = schema_map.get("resources", [])

    base_cache_dir = Path(args.cache_dir)
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    fetcher = Fetcher(sleep_s=args.sleep, cache_dir=str(base_cache_dir), force=args.force_fetch)

    range_targets: List[int] = []
    if args.lthing_range:
        try:
            start_s, end_s = args.lthing_range.split(",", 1)
            start_i = int(start_s.strip())
            end_i = int(end_s.strip())
            lo, hi = sorted((start_i, end_i))
            range_targets = list(range(lo, hi + 1))
        except Exception as e:
            raise SystemExit(f"Invalid --lthing-range '{args.lthing_range}': {e}") from e

    if args.all_lthing:
        targets = [(n, y) for n, y in list_lthing_sessions(fetcher) if y]
    elif range_targets:
        targets = []
        for lt in range_targets:
            try:
                _, y = discover_lthing_and_yfirlit(fetcher, lt)
                targets.append((lt, y))
            except Exception as e:
                print(f"[warn] skipping lthing {lt}: {e}")
    elif args.lthing is not None:
        lt, y = discover_lthing_and_yfirlit(fetcher, args.lthing)
        targets = [(lt, y)]
    else:
        lt, y = discover_current_lthing_and_yfirlit(fetcher)
        targets = [(lt, y)]

    for lthing, yfirlit in targets:
        cache_dir_for_lt = base_cache_dir / str(lthing)
        fetcher.cache_dir = str(cache_dir_for_lt)
        fetcher.cache_only_default = not args.force_fetch
        scoped_resources = resource_urls_for_lthing(resources, lthing, yfirlit)
        for r in scoped_resources:
            if "error" in r:
                continue
            name = r["name"]
            url = r["url"]
            record_path = r["record_path"]
            attr_map: Dict[str, str] = r["attr_map"]
            leaf_map: Dict[str, str] = r["leaf_map"]
            repeated_paths = set(r.get("repeated_leaf_paths", []))
            try:
                xml_bytes = fetcher.get(url, cache_only=not args.force_fetch)
            except Exception as e:
                print(f"[warn] cache miss/fetch failed for {name} (lthing {lthing}): {e}")
                continue
            try:
                root = parse_xml(xml_bytes, url)
            except Exception as e:
                print(f"[warn] parse failed for {name} (lthing {lthing}): {e}")
                continue

            records = iter_records(root, record_path)
            rows: List[Dict[str, Any]] = []
            for rec in records:
                row: Dict[str, Any] = {
                    "ingest_lthing": lthing,
                    "ingest_resource": name,
                    "source_url": url,
                }
                for k, v in attr_map.items():
                    row[v] = rec.attrib.get(k)
                leafs = iter_leaf_fields(rec)
                per_path: Dict[str, List[str]] = {}
                for p, v in leafs:
                    per_path.setdefault(p, []).append(v)
                for p, col in leaf_map.items():
                    vals = per_path.get(p)
                    if vals:
                        row[col] = vals if p in repeated_paths else vals[0]
                rows.append(row)

            if not rows:
                continue
            headers = sorted({k for row in rows for k in row.keys()})
            csv_path = outdir / str(lthing) / f"{r['table']}.csv"
            write_csv(csv_path, rows, headers)
            print(f"[ok] bronze cached {len(rows)} rows -> {csv_path}")

    print("Done.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
